{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMqWDc_m6rUC"
   },
   "source": [
    "\n",
    "##### Copyright 2019 Google LLC.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNhgka4UKNjf"
   },
   "outputs": [],
   "source": [
    "# Copyright 2019 Google LLC. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JndnmDMp66FL"
   },
   "source": [
    "# GANSynth Demo\n",
    "\n",
    "This notebook is a demo *GANSynth*, which generates audio with Generative Adversarial Networks. \n",
    "GANSynth learns to produce individual instrument notes like the [NSynth Dataset](https://magenta.tensorflow.org/datasets/nsynth). With pitch provided as a conditional attribute, the generator learns to use its latent space to represent different instrument timbres. This allows us to synthesize performances from MIDI files, either keeping the timbre constant, or interpolating between instruments over time.\n",
    "\n",
    "* [GANSynth ICLR paper](https://arxiv.org/abs/1809.11096)\n",
    "* [Audio Examples](http://goo.gl/magenta/gansynth-examples) \n",
    "\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/magentadata/papers/gansynth/figures/models.jpeg\" alt=\"GANSynth figure\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J2z5Sh6wZrP"
   },
   "source": [
    "## 1: Environment Setup\n",
    "\n",
    "\n",
    "This notebook synthesizes audio from uploaded MIDI files. There are two different flavors:\n",
    "* Interpolate between random instruments\n",
    "* Interpolate between two chosen instruments\n",
    "\n",
    "\n",
    "Have fun! And please feel free to hack this notebook to make your own creative interactions.\n",
    "\n",
    "### Instructions for running:\n",
    "\n",
    "* Make sure to use a GPU runtime, click:  __Runtime >> Change Runtime Type >> GPU__\n",
    "* Then press the **Play** button on the left of each of the cells\n",
    "* Double-click any of the cells to view the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwQleTRmCoE5",
    "outputId": "c8e424ea-17eb-4aa8-f918-a4df37e8e056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying data from GCS...\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  4922  100  4922    0     0  27729      0 --:--:-- --:--:-- --:--:-- 28616\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1083  100  1083    0     0  11388      0 --:--:-- --:--:-- --:--:-- 12306\n",
      "Collecting magenta\n",
      "  Using cached magenta-2.1.3-py3-none-any.whl (1.4 MB)\n",
      "Collecting sk-video\n",
      "  Using cached sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\n",
      "Collecting tensor2tensor\n",
      "  Using cached tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: tensorflow in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from magenta) (2.6.2)\n",
      "Collecting sox>=1.3.7\n",
      "  Using cached sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
      "Collecting tf-slim\n",
      "  Using cached tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Requirement already satisfied: numpy in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from magenta) (1.19.5)\n",
      "Collecting mir-eval>=0.4\n",
      "  Using cached mir_eval-0.7.tar.gz (90 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting imageio\n",
      "  Using cached imageio-2.19.5-py3-none-any.whl (3.4 MB)\n",
      "Collecting librosa<0.8.0,>=0.6.2\n",
      "  Using cached librosa-0.7.2.tar.gz (1.6 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting python-rtmidi<1.2,>=1.1\n",
      "  Using cached python-rtmidi-1.1.2.tar.gz (204 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=1.5.3 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from magenta) (3.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from magenta) (1.15.0)\n",
      "Collecting pygtrie>=2.3\n",
      "  Using cached pygtrie-2.5.0.tar.gz (39 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from magenta) (1.7.1)\n",
      "Collecting pretty-midi>=0.2.6\n",
      "  Using cached pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from magenta) (0.15.0)\n",
      "Requirement already satisfied: Pillow>=3.4.2 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from magenta) (9.2.0)\n",
      "Collecting dopamine-rl<=3.0.1\n",
      "  Using cached dopamine_rl-3.0.1-py3-none-any.whl (84 kB)\n",
      "Collecting scikit-image\n",
      "  Using cached scikit_image-0.19.3-cp39-cp39-macosx_12_0_arm64.whl (12.5 MB)\n",
      "Collecting dm-sonnet\n",
      "  Using cached dm_sonnet-2.0.0-py3-none-any.whl (254 kB)\n",
      "Collecting note-seq\n",
      "  Using cached note_seq-0.0.5-py3-none-any.whl (209 kB)\n",
      "Collecting tensorflow-datasets\n",
      "  Using cached tensorflow_datasets-4.6.0-py3-none-any.whl (4.3 MB)\n",
      "Requirement already satisfied: wheel in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from magenta) (0.37.1)\n",
      "Collecting numba<0.50\n",
      "  Using cached numba-0.49.1.tar.gz (2.0 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tensorflow-probability\n",
      "  Using cached tensorflow_probability-0.17.0-py2.py3-none-any.whl (6.5 MB)\n",
      "Collecting mido==1.2.6\n",
      "  Using cached mido-1.2.6-py2.py3-none-any.whl (69 kB)\n",
      "Collecting gym>=0.10.5\n",
      "  Using cached gym-0.25.0.tar.gz (720 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gin-config>=0.1.1\n",
      "  Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "Collecting opencv-python>=3.4.1.15\n",
      "  Using cached opencv_python-4.6.0.66-cp37-abi3-macosx_11_0_arm64.whl (30.0 MB)\n",
      "Requirement already satisfied: audioread>=2.0.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from librosa<0.8.0,>=0.6.2->magenta) (2.1.9)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from librosa<0.8.0,>=0.6.2->magenta) (1.0.1)\n",
      "Requirement already satisfied: joblib>=0.12 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from librosa<0.8.0,>=0.6.2->magenta) (1.1.0)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from librosa<0.8.0,>=0.6.2->magenta) (5.1.1)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from librosa<0.8.0,>=0.6.2->magenta) (0.3.1)\n",
      "Requirement already satisfied: soundfile>=0.9.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from librosa<0.8.0,>=0.6.2->magenta) (0.10.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from matplotlib>=1.5.3->magenta) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from matplotlib>=1.5.3->magenta) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from matplotlib>=1.5.3->magenta) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from matplotlib>=1.5.3->magenta) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from matplotlib>=1.5.3->magenta) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from matplotlib>=1.5.3->magenta) (21.3)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting llvmlite<=0.33.0.dev0,>=0.31.0.dev0\n",
      "  Using cached llvmlite-0.32.1.tar.gz (104 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from numba<0.50->magenta) (61.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from dm-sonnet->magenta) (1.12.1)\n",
      "Collecting tabulate>=0.7.5\n",
      "  Using cached tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Collecting dm-tree>=0.1.1\n",
      "  Using cached dm_tree-0.1.7-cp39-cp39-macosx_11_0_arm64.whl (105 kB)\n",
      "Requirement already satisfied: IPython in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from note-seq->magenta) (8.4.0)\n",
      "Collecting protobuf>=4.21.2\n",
      "  Using cached protobuf-4.21.3-cp37-abi3-macosx_10_9_universal2.whl (484 kB)\n",
      "Requirement already satisfied: attrs in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from note-seq->magenta) (21.4.0)\n",
      "Collecting intervaltree>=2.1.0\n",
      "  Using cached intervaltree-3.1.0-py2.py3-none-any.whl\n",
      "Collecting pandas>=0.18.1\n",
      "  Using cached pandas-1.4.3-cp39-cp39-macosx_11_0_arm64.whl (10.5 MB)\n",
      "Collecting pydub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting bokeh>=0.12.0\n",
      "  Using cached bokeh-2.4.3-py3-none-any.whl (18.5 MB)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Using cached PyWavelets-1.3.0-cp39-cp39-macosx_11_0_arm64.whl (4.3 MB)\n",
      "Collecting networkx>=2.2\n",
      "  Using cached networkx-2.8.5-py3-none-any.whl (2.0 MB)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Using cached tifffile-2022.5.4-py3-none-any.whl (195 kB)\n",
      "Collecting gunicorn\n",
      "  Using cached gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "Collecting tensorflow-gan\n",
      "  Using cached tensorflow_gan-2.1.0-py2.py3-none-any.whl (367 kB)\n",
      "Collecting flask\n",
      "  Using cached Flask-2.1.3-py3-none-any.whl (95 kB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.10.1-py3-none-any.whl (6.4 MB)\n",
      "Collecting tensorflow-probability\n",
      "  Using cached tensorflow_probability-0.7.0-py2.py3-none-any.whl (981 kB)\n",
      "Collecting tensorflow-addons\n",
      "  Using cached tensorflow_addons-0.17.1-cp39-cp39-macosx_11_0_arm64.whl (545 kB)\n",
      "Requirement already satisfied: h5py in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensor2tensor->magenta) (3.1.0)\n",
      "Requirement already satisfied: requests in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensor2tensor->magenta) (2.28.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kfac\n",
      "  Using cached kfac-0.2.4-py2.py3-none-any.whl (193 kB)\n",
      "Collecting google-api-python-client\n",
      "  Using cached google_api_python_client-2.54.0-py2.py3-none-any.whl (8.8 MB)\n",
      "Collecting mesh-tensorflow\n",
      "  Using cached mesh_tensorflow-0.1.21-py3-none-any.whl (385 kB)\n",
      "Collecting oauth2client\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Collecting pypng\n",
      "  Using cached pypng-0.20220715.0-py3-none-any.whl (58 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "Collecting bz2file\n",
      "  Using cached bz2file-0.98.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gevent\n",
      "  Using cached gevent-21.12.0-cp39-cp39-macosx_12_0_arm64.whl\n",
      "Collecting cloudpickle>=0.6.1\n",
      "  Using cached cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensorflow->magenta) (1.1.2)\n",
      "Requirement already satisfied: keras<2.7,>=2.6.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensorflow->magenta) (2.6.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensorflow->magenta) (3.7.4.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensorflow->magenta) (2.6.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensorflow->magenta) (1.12)\n",
      "Requirement already satisfied: gast==0.4.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensorflow->magenta) (0.4.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensorflow->magenta) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensorflow->magenta) (3.3.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensorflow->magenta) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.7,>=2.6.0 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensorflow->magenta) (2.6.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensorflow->magenta) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.42 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from tensorflow->magenta) (1.42.0)\n",
      "Collecting etils[epath]\n",
      "  Using cached etils-0.6.0-py3-none-any.whl (98 kB)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.9.0-py3-none-any.whl (51 kB)\n",
      "Collecting toml\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting dill\n",
      "  Using cached dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: tornado>=5.1 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from bokeh>=0.12.0->note-seq->magenta) (6.1)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /Users/jesse/miniconda3/envs/MLearning/lib/python3.9/site-packages (from bokeh>=0.12.0->note-seq->magenta) (3.1.2)\n",
      "Collecting PyYAML>=3.10\n",
      "  Using cached PyYAML-6.0-cp39-cp39-macosx_11_0_arm64.whl (173 kB)\n",
      "Collecting bokeh>=0.12.0\n",
      "  Using cached bokeh-2.4.2-py3-none-any.whl (18.5 MB)\n",
      "  Using cached bokeh-2.4.1-py3-none-any.whl (18.5 MB)\n",
      "  Using cached bokeh-2.4.0-py3-none-any.whl (18.4 MB)\n",
      "  Using cached bokeh-2.3.3.tar.gz (10.7 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting importlib-metadata>=4.8.0\n",
      "  Using cached importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Using cached gym_notices-0.0.7-py3-none-any.whl (2.7 kB)\n",
      "Collecting sortedcontainers<3.0,>=2.0\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting opencv-python>=3.4.1.15\n",
      "  Using cached opencv_python-4.5.5.64-cp37-abi3-macosx_11_0_arm64.whl (29.9 MB)\n",
      "  Using cached opencv_python-4.5.5.62-cp37-abi3-macosx_11_0_arm64.whl (27.9 MB)\n",
      "  Using cached opencv_python-4.5.4.60-cp39-cp39-macosx_11_0_arm64.whl (27.9 MB)\n",
      "  Using cached opencv_python-4.5.4.58-cp39-cp39-macosx_11_0_arm64.whl (27.9 MB)\n",
      "  Using cached opencv_python-4.5.3.56-cp39-cp39-macosx_11_0_arm64.whl (10.7 MB)\n",
      "  Using cached opencv-python-4.5.1.48.tar.gz (88.3 MB)\n",
      "  Installing build dependencies ... \u001b[?25l\\"
     ]
    }
   ],
   "source": [
    "#@title Install\n",
    "\n",
    "#@markdown Install magenta, define some helper functions, and download the model. This transfers a lot of data and _should take a minute or two_.\n",
    "\n",
    "# Install Magenta\n",
    "print('Copying data from GCS...')\n",
    "!rm -r ./gansynth\n",
    "!mkdir ./gansynth\n",
    "!mkdir ./gansynth/midi\n",
    "!mkdir ./gansynth/samples\n",
    "\n",
    "# Get default MIDI (Bach Prelude)\n",
    "!curl -o ./gansynth/midi/bach.mid http://www.jsbach.net/midi/cs1-1pre.mid\n",
    "MIDI_SONG_DEFAULT = './gansynth/midi/bach.mid'\n",
    "!curl -o ./gansynth/midi/riff-default.mid http://storage.googleapis.com/magentadata/papers/gansynth/midi/arp.mid\n",
    "MIDI_RIFF_DEFAULT = './gansynth/midi/riff-default.mid'\n",
    "\n",
    "!pip install -U magenta\n",
    "\n",
    "import os\n",
    "from google.colab import files\n",
    "import librosa\n",
    "from magenta.models.nsynth.utils import load_audio\n",
    "from magenta.models.gansynth.lib import flags as lib_flags\n",
    "from magenta.models.gansynth.lib import generate_util as gu\n",
    "from magenta.models.gansynth.lib import model as lib_model\n",
    "from magenta.models.gansynth.lib import util\n",
    "import matplotlib.pyplot as plt\n",
    "import note_seq\n",
    "from note_seq.notebook_utils import colab_play as play\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# File IO\n",
    "download = files.download\n",
    "\n",
    "def upload():\n",
    "  '''Upload a .wav file.'''\n",
    "  filemap = files.upload()\n",
    "  file_list = []\n",
    "  for key, value in filemap.items():\n",
    "    fname = os.path.join('./gansynth/midi', key)\n",
    "    with open(fname, 'wb') as f:\n",
    "      f.write(value)\n",
    "      print('Writing {}'.format(fname))\n",
    "    file_list.append(fname)\n",
    "  return file_list\n",
    "\n",
    "# GLOBALS\n",
    "CKPT_DIR = 'gs://magentadata/models/gansynth/acoustic_only'\n",
    "output_dir = './gansynth/samples'\n",
    "BATCH_SIZE = 16\n",
    "SR = 16000\n",
    "\n",
    "# Make an output directory if it doesn't exist\n",
    "OUTPUT_DIR = util.expand_path(output_dir)\n",
    "if not tf.gfile.Exists(OUTPUT_DIR):\n",
    "  tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "\n",
    "# Load the model\n",
    "tf.reset_default_graph()\n",
    "flags = lib_flags.Flags({\n",
    "    'batch_size_schedule': [BATCH_SIZE],\n",
    "    'tfds_data_dir': \"gs://tfds-data/datasets\",\n",
    "})\n",
    "model = lib_model.Model.load_from_path(CKPT_DIR, flags)\n",
    "\n",
    "# Helper functions\n",
    "def load_midi(midi_path, min_pitch=36, max_pitch=84):\n",
    "  \"\"\"Load midi as a notesequence.\"\"\"\n",
    "  midi_path = util.expand_path(midi_path)\n",
    "  ns = note_seq.midi_file_to_sequence_proto(midi_path)\n",
    "  pitches = np.array([n.pitch for n in ns.notes])\n",
    "  velocities = np.array([n.velocity for n in ns.notes])\n",
    "  start_times = np.array([n.start_time for n in ns.notes])\n",
    "  end_times = np.array([n.end_time for n in ns.notes])\n",
    "  valid = np.logical_and(pitches >= min_pitch, pitches <= max_pitch)\n",
    "  notes = {'pitches': pitches[valid],\n",
    "           'velocities': velocities[valid],\n",
    "           'start_times': start_times[valid],\n",
    "           'end_times': end_times[valid]}\n",
    "  return ns, notes\n",
    "\n",
    "def get_envelope(t_note_length, t_attack=0.010, t_release=0.3, sr=16000):\n",
    "  \"\"\"Create an attack sustain release amplitude envelope.\"\"\"\n",
    "  t_note_length = min(t_note_length, 3.0)\n",
    "  i_attack = int(sr * t_attack)\n",
    "  i_sustain = int(sr * t_note_length)\n",
    "  i_release = int(sr * t_release)\n",
    "  i_tot = i_sustain + i_release  # attack envelope doesn't add to sound length\n",
    "  envelope = np.ones(i_tot)\n",
    "  # Linear attack\n",
    "  envelope[:i_attack] = np.linspace(0.0, 1.0, i_attack)\n",
    "  # Linear release\n",
    "  envelope[i_sustain:i_tot] = np.linspace(1.0, 0.0, i_release)\n",
    "  return envelope\n",
    "\n",
    "def combine_notes(audio_notes, start_times, end_times, velocities, sr=16000):\n",
    "  \"\"\"Combine audio from multiple notes into a single audio clip.\n",
    "\n",
    "  Args:\n",
    "    audio_notes: Array of audio [n_notes, audio_samples].\n",
    "    start_times: Array of note starts in seconds [n_notes].\n",
    "    end_times: Array of note ends in seconds [n_notes].\n",
    "    sr: Integer, sample rate.\n",
    "\n",
    "  Returns:\n",
    "    audio_clip: Array of combined audio clip [audio_samples]\n",
    "  \"\"\"\n",
    "  n_notes = len(audio_notes)\n",
    "  clip_length = end_times.max() + 3.0\n",
    "  audio_clip = np.zeros(int(clip_length) * sr)\n",
    "\n",
    "  for t_start, t_end, vel, i in zip(start_times, end_times, velocities, range(n_notes)):\n",
    "    # Generate an amplitude envelope\n",
    "    t_note_length = t_end - t_start\n",
    "    envelope = get_envelope(t_note_length)\n",
    "    length = len(envelope)\n",
    "    audio_note = audio_notes[i, :length] * envelope\n",
    "    # Normalize\n",
    "    audio_note /= audio_note.max()\n",
    "    audio_note *= (vel / 127.0)\n",
    "    # Add to clip buffer\n",
    "    clip_start = int(t_start * sr)\n",
    "    clip_end = clip_start + length\n",
    "    audio_clip[clip_start:clip_end] += audio_note\n",
    "\n",
    "  # Normalize\n",
    "  audio_clip /= audio_clip.max()\n",
    "  audio_clip /= 2.0\n",
    "  return audio_clip\n",
    "\n",
    "# Plotting tools\n",
    "def specplot(audio_clip):\n",
    "  p_min = np.min(36)\n",
    "  p_max = np.max(84)\n",
    "  f_min = librosa.midi_to_hz(p_min)\n",
    "  f_max = 2 * librosa.midi_to_hz(p_max)\n",
    "  octaves = int(np.ceil(np.log2(f_max) - np.log2(f_min)))\n",
    "  bins_per_octave = 36\n",
    "  n_bins = int(bins_per_octave * octaves)\n",
    "  C = librosa.cqt(audio_clip, sr=SR, hop_length=2048, fmin=f_min, n_bins=n_bins, bins_per_octave=bins_per_octave)\n",
    "  power = 10 * np.log10(np.abs(C)**2 + 1e-6)\n",
    "  plt.matshow(power[::-1, 2:-2], aspect='auto', cmap=plt.cm.magma)\n",
    "  plt.yticks([])\n",
    "  plt.xticks([])\n",
    "\n",
    "print('And...... Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFlCLrPyxI2Q"
   },
   "source": [
    "## 2(a): Random Interpolation\n",
    "\n",
    "These cells take the MIDI for a full song and interpolate between several random latent vectors (equally spaced in time) over the whole song. The result sounds like instruments that slowly and smoothly morph between each other.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "yDq-YKHoYhkz"
   },
   "outputs": [],
   "source": [
    "#@title Choose a MIDI file\n",
    "\n",
    "#@markdown Upload a MIDI file _(.mid, single instrument)_ for audio synthesis, or use the provided default. You can find lots of free MIDI files [online](http://www.midiworld.com/files/).\n",
    "\n",
    "midi_file = \"Bach Prelude (Default)\" #@param [\"Bach Prelude (Default)\", \"Upload your own\"]\n",
    "\n",
    "midi_path = MIDI_SONG_DEFAULT\n",
    "if midi_file == \"Upload your own\":\n",
    "  try:\n",
    "    file_list = upload()\n",
    "    midi_path = file_list[0]\n",
    "  except Exception as e:\n",
    "    print('Upload Cancelled')\n",
    "\n",
    "ns, notes = load_midi(midi_path)\n",
    "print('Loaded {}'.format(midi_path))\n",
    "note_seq.plot_sequence(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "EwFvRyg6xSoz"
   },
   "outputs": [],
   "source": [
    "#@title Generate random interpolation ðŸŽµ\n",
    "#@markdown Select the number of seconds to take in interpolating between each random instrument. Larger numbers will have slower and smoother interpolations.\n",
    "\n",
    "seconds_per_instrument = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "\n",
    "#@markdown This cell plays and displays a [Constant-Q spectrogram](https://en.wikipedia.org/wiki/Constant-Q_transform) of the synthesized audio.\n",
    "\n",
    "# Distribute latent vectors linearly in time\n",
    "z_instruments, t_instruments = gu.get_random_instruments(\n",
    "    model, notes['end_times'][-1], secs_per_instrument=seconds_per_instrument)\n",
    "\n",
    "# Get latent vectors for each note\n",
    "z_notes = gu.get_z_notes(notes['start_times'], z_instruments, t_instruments)\n",
    "\n",
    "# Generate audio for each note\n",
    "print('Generating {} samples...'.format(len(z_notes)))\n",
    "audio_notes = model.generate_samples_from_z(z_notes, notes['pitches'])\n",
    "\n",
    "# Make a single audio clip\n",
    "audio_clip = combine_notes(audio_notes,\n",
    "                           notes['start_times'],\n",
    "                           notes['end_times'],\n",
    "                           notes['velocities'])\n",
    "\n",
    "# Play the audio\n",
    "print('\\nAudio:')\n",
    "play(audio_clip, sample_rate=SR)\n",
    "print('CQT Spectrogram:')\n",
    "specplot(audio_clip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "EX-tM_Gp3X6W"
   },
   "outputs": [],
   "source": [
    "#@title Download \n",
    "#@markdown Get the .wav file (optional)\n",
    "\n",
    "# Write the file\n",
    "fname = os.path.join(output_dir, 'generated_clip.wav')\n",
    "gu.save_wav(audio_clip, fname)\n",
    "download(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scqmGhen5VVv"
   },
   "source": [
    "## 2(b): You Choose the Interpolation\n",
    "\n",
    "These cells allow you to choose two latent vectors and interpolate between them over a MIDI clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Jg7AlosuYm3M"
   },
   "outputs": [],
   "source": [
    "#@title Choose a MIDI file\n",
    "\n",
    "#@markdown Upload a MIDI file _(.mid, single instrument)_ for audio synthesis, or use the provided default.\n",
    "\n",
    "midi_file = \"Arpeggio (Default)\" #@param [\"Arpeggio (Default)\", \"Upload your own\"]\n",
    "\n",
    "midi_path = MIDI_RIFF_DEFAULT\n",
    "if midi_file == \"Upload your own\":\n",
    "  try:\n",
    "    file_list = upload()\n",
    "    midi_path = file_list[0]\n",
    "    ns, notes_2 = load_midi(midi_path)\n",
    "  except Exception as e:\n",
    "    print('Upload Cancelled')\n",
    "else:\n",
    "  # Load Default, but slow it down 30%\n",
    "  ns, notes_2 = load_midi(midi_path)\n",
    "  notes_2['start_times'] *= 1.3\n",
    "  notes_2['end_times'] *= 1.3\n",
    "\n",
    "\n",
    "print('Loaded {}'.format(midi_path))\n",
    "note_seq.plot_sequence(ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lTWOFFom6tMr"
   },
   "outputs": [],
   "source": [
    "#@title Sample some random instruments\n",
    "\n",
    "number_of_random_instruments = 10 #@param {type:\"slider\", min:4, max:16, step:1}\n",
    "pitch_preview = 60\n",
    "n_preview = number_of_random_instruments\n",
    "\n",
    "pitches_preview = [pitch_preview] * n_preview\n",
    "z_preview = model.generate_z(n_preview)\n",
    "\n",
    "audio_notes = model.generate_samples_from_z(z_preview, pitches_preview)\n",
    "for i, audio_note in enumerate(audio_notes):\n",
    "  print(\"Instrument: {}\".format(i))\n",
    "  play(audio_note, sample_rate=16000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Vw9-tp6J5VV1"
   },
   "outputs": [],
   "source": [
    "#@title Generate custom interpolation ðŸŽµ\n",
    "##@markdown Using the instrument numbers from the cell above, pick two instruments to interpolate between over the clip.\n",
    "\n",
    "#@markdown Using the instrument numbers from the cell above, create a list of instruments to interpolate between. You can repeat instruments.\n",
    "\n",
    "instruments = [0, 2, 4, 0] #@param\n",
    "\n",
    "#@markdown For each instrument in the list above, place it at a relative time [0-1.0] in the clip. __The list of times must _always increase_ and start at 0 and end at 1.0__.\n",
    "\n",
    "times = [0, 0.3, 0.6, 1.0] #@param\n",
    "\n",
    "# Force endpoints\n",
    "times[0] = -0.001\n",
    "times[-1] = 1.0\n",
    "\n",
    "#@markdown This cell plays and displays a [Constant-Q spectrogram](https://en.wikipedia.org/wiki/Constant-Q_transform) of the synthesized audio.\n",
    "\n",
    "z_instruments = np.array([z_preview[i] for i in instruments])\n",
    "t_instruments = np.array([notes_2['end_times'][-1] * t for t in times])\n",
    "\n",
    "# Get latent vectors for each note\n",
    "z_notes = gu.get_z_notes(notes_2['start_times'], z_instruments, t_instruments)\n",
    "\n",
    "# Generate audio for each note\n",
    "print('Generating {} samples...'.format(len(z_notes)))\n",
    "audio_notes = model.generate_samples_from_z(z_notes, notes_2['pitches'])\n",
    "\n",
    "# Make a single audio clip\n",
    "audio_clip = combine_notes(audio_notes,\n",
    "                           notes_2['start_times'],\n",
    "                           notes_2['end_times'],\n",
    "                           notes_2['velocities'])\n",
    "\n",
    "# Play the audio\n",
    "print('\\nAudio:')\n",
    "play(audio_clip, sample_rate=SR)\n",
    "print('CQT Spectrogram:')\n",
    "specplot(audio_clip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "DCA9o-PI5VV_"
   },
   "outputs": [],
   "source": [
    "#@title Download\n",
    "#@markdown Get the .wav file (optional)\n",
    "\n",
    "# Write the file\n",
    "fname = os.path.join(output_dir, 'generated_clip.wav')\n",
    "gu.save_wav(audio_clip, fname)\n",
    "download(fname)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hMqWDc_m6rUC"
   ],
   "name": "gansynth_external.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
